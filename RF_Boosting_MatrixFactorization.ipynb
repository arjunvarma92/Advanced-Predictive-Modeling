{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">MIS 382N: Advanced Predictive Modeling</p>\n",
    "# <p style=\"text-align: center;\">Assignment 5</p>\n",
    "## <p style=\"text-align: center;\">Total points: 50 </p>\n",
    "## <p style=\"text-align: center;\">Due: Mon, November 28</p>\n",
    "\n",
    "\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. Please submit **only one** ipynb file from each group, and include the names of all the group members. Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Random Forest vs Boosting - Regression (15pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we will compare performance of different ensemble methods for regression problems: [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html), [Gradient Boosting Regressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) (GBR), and [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor). Board game data set from DataQuest will be used (you can download data from Canvas: 'games.csv').\n",
    "\n",
    "1. (1) Load the data, (2) remove duplicate rows, (3) remove features of type string (object in pandas), and (4) replace missing values by mean of each column. Then, partition data into features (X) and the target label (y) for regression task. We want to predict the *average_rating*. Use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split data into training and testing: test_size=0.33, random_state=42. (1pt)\n",
    "\n",
    "2. Use a [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) to predict average_rating. Find the best parameters (including *n_estimators*) using [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Report the accuracy of your model in terms of RMSE. (4pts)\n",
    "\n",
    "3. Use [Gradient Boosting Regressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) (GBR), and [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor) for predicting the targets. Again, find the best parameters (including *n_estimators,* and* learning_rate*), and report corresponding RMSE for each algorithm. (8pts)\n",
    "\n",
    "4. Which model did you expect to be more accurate in predicting the targets? Why? Did your observation match this expectation? (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,GridSearchCV)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import (RandomForestRegressor,GradientBoostingRegressor,AdaBoostRegressor)\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) (1) Load the data, (2) remove duplicate rows, (3) remove features of type string (object in pandas), and (4) replace missing values by mean of each column. Then, partition data into features (X) and the target label (y) for regression task. We want to predict the average_rating. Use train_test_split to split data into training and testing: test_size=0.33, random_state=42. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows,columns= (81312, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>yearpublished</th>\n",
       "      <th>minplayers</th>\n",
       "      <th>maxplayers</th>\n",
       "      <th>playingtime</th>\n",
       "      <th>minplaytime</th>\n",
       "      <th>maxplaytime</th>\n",
       "      <th>minage</th>\n",
       "      <th>users_rated</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>bayes_average_rating</th>\n",
       "      <th>total_owners</th>\n",
       "      <th>total_traders</th>\n",
       "      <th>total_wanters</th>\n",
       "      <th>total_wishers</th>\n",
       "      <th>total_comments</th>\n",
       "      <th>total_weights</th>\n",
       "      <th>average_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12333</td>\n",
       "      <td>boardgame</td>\n",
       "      <td>Twilight Struggle</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20113</td>\n",
       "      <td>8.33774</td>\n",
       "      <td>8.22186</td>\n",
       "      <td>26647</td>\n",
       "      <td>372</td>\n",
       "      <td>1219</td>\n",
       "      <td>5865</td>\n",
       "      <td>5347</td>\n",
       "      <td>2562</td>\n",
       "      <td>3.4785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120677</td>\n",
       "      <td>boardgame</td>\n",
       "      <td>Terra Mystica</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14383</td>\n",
       "      <td>8.28798</td>\n",
       "      <td>8.14232</td>\n",
       "      <td>16519</td>\n",
       "      <td>132</td>\n",
       "      <td>1586</td>\n",
       "      <td>6277</td>\n",
       "      <td>2526</td>\n",
       "      <td>1423</td>\n",
       "      <td>3.8939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102794</td>\n",
       "      <td>boardgame</td>\n",
       "      <td>Caverna: The Cave Farmers</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9262</td>\n",
       "      <td>8.28994</td>\n",
       "      <td>8.06886</td>\n",
       "      <td>12230</td>\n",
       "      <td>99</td>\n",
       "      <td>1476</td>\n",
       "      <td>5600</td>\n",
       "      <td>1700</td>\n",
       "      <td>777</td>\n",
       "      <td>3.7761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25613</td>\n",
       "      <td>boardgame</td>\n",
       "      <td>Through the Ages: A Story of Civilization</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13294</td>\n",
       "      <td>8.20407</td>\n",
       "      <td>8.05804</td>\n",
       "      <td>14343</td>\n",
       "      <td>362</td>\n",
       "      <td>1084</td>\n",
       "      <td>5075</td>\n",
       "      <td>3378</td>\n",
       "      <td>1642</td>\n",
       "      <td>4.1590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3076</td>\n",
       "      <td>boardgame</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>39883</td>\n",
       "      <td>8.14261</td>\n",
       "      <td>8.04524</td>\n",
       "      <td>44362</td>\n",
       "      <td>795</td>\n",
       "      <td>861</td>\n",
       "      <td>5414</td>\n",
       "      <td>9173</td>\n",
       "      <td>5213</td>\n",
       "      <td>3.2943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id       type                                       name  \\\n",
       "0   12333  boardgame                          Twilight Struggle   \n",
       "1  120677  boardgame                              Terra Mystica   \n",
       "2  102794  boardgame                  Caverna: The Cave Farmers   \n",
       "3   25613  boardgame  Through the Ages: A Story of Civilization   \n",
       "4    3076  boardgame                                Puerto Rico   \n",
       "\n",
       "   yearpublished  minplayers  maxplayers  playingtime  minplaytime  \\\n",
       "0         2005.0         2.0         2.0        180.0        180.0   \n",
       "1         2012.0         2.0         5.0        150.0         60.0   \n",
       "2         2013.0         1.0         7.0        210.0         30.0   \n",
       "3         2006.0         2.0         4.0        240.0        240.0   \n",
       "4         2002.0         2.0         5.0        150.0         90.0   \n",
       "\n",
       "   maxplaytime  minage  users_rated  average_rating  bayes_average_rating  \\\n",
       "0        180.0    13.0        20113         8.33774               8.22186   \n",
       "1        150.0    12.0        14383         8.28798               8.14232   \n",
       "2        210.0    12.0         9262         8.28994               8.06886   \n",
       "3        240.0    12.0        13294         8.20407               8.05804   \n",
       "4        150.0    12.0        39883         8.14261               8.04524   \n",
       "\n",
       "   total_owners  total_traders  total_wanters  total_wishers  total_comments  \\\n",
       "0         26647            372           1219           5865            5347   \n",
       "1         16519            132           1586           6277            2526   \n",
       "2         12230             99           1476           5600            1700   \n",
       "3         14343            362           1084           5075            3378   \n",
       "4         44362            795            861           5414            9173   \n",
       "\n",
       "   total_weights  average_weight  \n",
       "0           2562          3.4785  \n",
       "1           1423          3.8939  \n",
       "2            777          3.7761  \n",
       "3           1642          4.1590  \n",
       "4           5213          3.2943  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the data\n",
    "data = pd.read_csv('games.csv')\n",
    "print \"Rows,columns=\",data.shape\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing duplicates: (79463, 20) \n",
      "\n",
      "After removing columns of string object: (79463, 18) \n",
      "\n",
      "Missing values present?: False\n"
     ]
    }
   ],
   "source": [
    "# Removing duplicate rows\n",
    "data = data.drop_duplicates()\n",
    "print \"After removing duplicates:\",data.shape,\"\\n\"\n",
    "\n",
    "# Removing columns of string object\n",
    "data = data[data.dtypes[data.dtypes != \"object\"].index]\n",
    "print \"After removing columns of string object:\",data.shape,\"\\n\"\n",
    "\n",
    "# Replace missing values by mean\n",
    "data = data.fillna(data.mean())\n",
    "print \"Missing values present?:\",data.isnull().values.any()\n",
    "\n",
    "# Splitting into train and test\n",
    "X = data.ix[:, data.columns != 'average_rating']\n",
    "y = data['average_rating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Use a Random Forest to predict average_rating. Find the best parameters (including n_estimators) using GridSearchCV. Report the accuracy of your model in terms of RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up the model\n",
    "parameters = {'n_estimators':[5,10,15],'max_depth':[None,5,10,15],'min_samples_split':[2,.1,.2],'min_samples_leaf':[2,.01,.02]}\n",
    "rf_regressor = RandomForestRegressor()\n",
    "\n",
    "# Optimizing parameters\n",
    "grid_search_model = GridSearchCV(rf_regressor,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 15], 'min_samples_split': [2, 0.1, 0.2], 'max_depth': [None, 5, 10, 15], 'min_samples_leaf': [2, 0.01, 0.02]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model on the data\n",
    "grid_search_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 15,\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 15}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing out the optimal parameters\n",
    "grid_search_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE using RF:  1.00461088488\n"
     ]
    }
   ],
   "source": [
    "# Predicting on test\n",
    "pred_test = grid_search_model.predict(X_test)\n",
    "rmse = sqrt(mean_squared_error(y_test, pred_test))\n",
    "print \"RMSE using RF: \",rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Use Gradient Boosting Regressor (GBR), and AdaBoost for predicting the targets. Again, find the best parameters (including n_estimators, and learning_rate), and report corresponding RMSE for each algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up the model\n",
    "parameters_gb = {'loss':('ls','huber', 'quantile'),'learning_rate': [.05,.1,.15],'n_estimators':[50,100,150],'max_depth':[2,3,5],'min_samples_split':[2,.1,.2],'min_samples_leaf':[1,.01,.02]}\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Optimizing parameters\n",
    "grid_search_model_gb = GridSearchCV(gb_regressor,parameters_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "             warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'loss': ('ls', 'huber', 'quantile'), 'learning_rate': [0.05, 0.1, 0.15], 'min_samples_leaf': [1, 0.01, 0.02], 'n_estimators': [50, 100, 150], 'min_samples_split': [2, 0.1, 0.2], 'max_depth': [2, 3, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model on the data\n",
    "grid_search_model_gb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.15,\n",
       " 'loss': 'huber',\n",
       " 'max_depth': 5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 150}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing out the optimal parameters\n",
    "grid_search_model_gb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE using GBR:  0.992558014884\n"
     ]
    }
   ],
   "source": [
    "# Predicting on test\n",
    "pred_test = grid_search_model_gb.predict(X_test)\n",
    "rmse = sqrt(mean_squared_error(y_test, pred_test))\n",
    "print \"RMSE using GBR: \",rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting up the model\n",
    "parameters_ada = {'loss':('linear','square'),'learning_rate': [.5,1,1.5],'n_estimators':[20,50,100]}\n",
    "ada_regressor = AdaBoostRegressor()\n",
    "\n",
    "# Optimizing parameters\n",
    "grid_search_model_ada = GridSearchCV(ada_regressor,parameters_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
       "         n_estimators=50, random_state=None),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [20, 50, 100], 'loss': ('linear', 'square'), 'learning_rate': [0.5, 1, 1.5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model on the data\n",
    "grid_search_model_ada.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.5, 'loss': 'linear', 'n_estimators': 50}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing out the optimal parameters\n",
    "grid_search_model_ada.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE using Adaboost:  1.1366529744\n"
     ]
    }
   ],
   "source": [
    "# Predicting on test\n",
    "pred_test = grid_search_model_ada.predict(X_test)\n",
    "rmse = sqrt(mean_squared_error(y_test, pred_test))\n",
    "print \"RMSE using Adaboost: \",rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Which model did you expect to be more accurate in predicting the targets? Why? Did your observation match this expectation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is often more accurate than random forests since it reduces both bias and variance. Since Adaboost is a variation of gradient boosting that upweighs misclassified points, we expected it to perform the best.\n",
    "\n",
    "In this scenario, GBM performed the best possibly because the Adaboost algorithm overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Random Forest vs Boosting - Classification (15 pts)\n",
    "In this question, we will compare performance of different ensemble methods for classification problems: [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), [Gradient Boosting Decision Tree](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) (GBDT), and [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier). [Spam Classification Data](https://archive.ics.uci.edu/ml/datasets/Spambase) of UCI will be used (you can download data from Canvas: 'spam_uci.csv'). Don't worry about column names. The last column represents target label, 1 if spam and zero otherwise.\n",
    "\n",
    "1. Load the data and partition it into features (X) and the target label (y) for classification task. Then, use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split data into training and testing: test_size=0.33, random_state=42. (1pt)\n",
    "\n",
    "2. Use a [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to classify whether an email is spam. Find the best parameters (including *n_estimators* and *criterion*) using [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Report your testing accuracy ([accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)) and [roc_auc_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score). You will need [predict_proba](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict_proba) for roc_auc_score. (4pts)\n",
    "\n",
    "3. Use [Gradient Boosting Decision Tree](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) (GBDT), and [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) for the spam classification problem. Again, find the best parameters (including *n_estimators, learning_rate,* and *max_depth (GBDT only)*), and report corresponding accuracy_score and roc_auc_score on the test data for each algorithm. (8pts)\n",
    "\n",
    "4. Point out one advantage and one disadvantage of Random Forest compared to GBDT (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,GridSearchCV)\n",
    "from sklearn.metrics import (accuracy_score,roc_auc_score)\n",
    "from sklearn.ensemble import (RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier)\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Load the data and partition it into features (X) and the target label (y) for classification task. Then, use train_test_split to split data into training and testing: test_size=0.33, random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     0     1     2    3     4     5     6     7     8 ...    48  \\\n",
       "0           0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00 ...  0.00   \n",
       "1           1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00 ...  0.00   \n",
       "2           2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64 ...  0.01   \n",
       "3           3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31 ...  0.00   \n",
       "4           4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31 ...  0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  57  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the data\n",
    "spam_uci = pd.read_csv('spam_uci.csv')\n",
    "spam_uci[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Splitting into train and test\n",
    "X = spam_uci.ix[:, spam_uci.columns != '57']\n",
    "y = spam_uci['57']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Use a Random Forest to classify whether an email is spam. Find the best parameters (including n_estimators and criterion) using GridSearchCV. Report your testing accuracy (accuracy_score) and roc_auc_score. You will need predict_proba for roc_auc_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up the model\n",
    "parameters = {'n_estimators':[5,10,15],'criterion':('gini','entropy'),'max_depth':[None,5,10,15],'min_samples_split':[2,.1,.2],'min_samples_leaf':[2,.01,.02]}\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Optimizing parameters\n",
    "grid_search_model_class = GridSearchCV(rf_classifier,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [5, 10, 15], 'min_samples_split': [2, 0.1, 0.2], 'criterion': ('gini', 'entropy'), 'max_depth': [None, 5, 10, 15], 'min_samples_leaf': [2, 0.01, 0.02]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model on the data\n",
    "grid_search_model_class.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 15,\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 10}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing out the optimal parameters\n",
    "grid_search_model_class.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Actual_class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted_class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>883</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Actual_class       0    1\n",
       "Predicted_class          \n",
       "0                883    1\n",
       "1                  3  632"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is:  0.997366688611 \n",
      "\n",
      "The ROC AUC is:  0.999987518677\n"
     ]
    }
   ],
   "source": [
    "# Predicting on test\n",
    "pred_test = grid_search_model_class.predict(X_test)\n",
    "\n",
    "# Getting performance stats\n",
    "display(pd.crosstab(pred_test,y_test,rownames = [\"Predicted_class\"], colnames = [\"Actual_class\"]))\n",
    "print \"The accuracy is: \", accuracy_score(y_test, pred_test),\"\\n\"\n",
    "print \"The ROC AUC is: \",roc_auc_score(y_test, grid_search_model_class.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Use Gradient Boosting Decision Tree (GBDT), and AdaBoost for the spam classification problem. Again, find the best parameters (including n_estimators, learning_rate, and max_depth (GBDT only)), and report corresponding accuracy_score and roc_auc_score on the test data for each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up the model\n",
    "parameters_gb = {'loss':('deviance','exponential'),'learning_rate': [.05,.1,.15],'n_estimators':[50,100,150],'max_depth':[2,3,5],'min_samples_split':[2,.1,.2],'min_samples_leaf':[1,.01,.02]}\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Optimizing parameters\n",
    "grid_search_model_class_gb = GridSearchCV(gb_classifier,parameters_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=100, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'loss': ('deviance', 'exponential'), 'learning_rate': [0.05, 0.1, 0.15], 'min_samples_leaf': [1, 0.01, 0.02], 'n_estimators': [50, 100, 150], 'min_samples_split': [2, 0.1, 0.2], 'max_depth': [2, 3, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model on the data\n",
    "grid_search_model_class_gb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05,\n",
       " 'loss': 'deviance',\n",
       " 'max_depth': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 50}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing out the optimal parameters\n",
    "grid_search_model_class_gb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Actual_class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted_class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>884</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Actual_class       0    1\n",
       "Predicted_class          \n",
       "0                884    0\n",
       "1                  2  633"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is:  0.998683344305 \n",
      "\n",
      "The ROC AUC is:  0.998871331828\n"
     ]
    }
   ],
   "source": [
    "# Predicting on test\n",
    "pred_test = grid_search_model_class_gb.predict(X_test)\n",
    "\n",
    "# Getting performance stats\n",
    "display(pd.crosstab(pred_test,y_test,rownames = [\"Predicted_class\"], colnames = [\"Actual_class\"]))\n",
    "print \"The accuracy is: \", accuracy_score(y_test, pred_test),\"\\n\"\n",
    "print \"The ROC AUC is: \",roc_auc_score(y_test, grid_search_model_class_gb.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up the model\n",
    "parameters_ada = {'algorithm':('SAMME','SAMME.R'),'learning_rate': [.5,1,1.5],'n_estimators':[20,50,100]}\n",
    "ada_classifier = AdaBoostClassifier()\n",
    "\n",
    "# Optimizing parameters\n",
    "grid_search_model_class_ada = GridSearchCV(ada_classifier,parameters_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [20, 50, 100], 'learning_rate': [0.5, 1, 1.5], 'algorithm': ('SAMME', 'SAMME.R')},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model on the data\n",
    "grid_search_model_class_ada.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'SAMME', 'learning_rate': 0.5, 'n_estimators': 20}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing out the optimal parameters\n",
    "grid_search_model_class_ada.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Actual_class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted_class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>884</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Actual_class       0    1\n",
       "Predicted_class          \n",
       "0                884    0\n",
       "1                  2  633"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is:  0.998683344305 \n",
      "\n",
      "The ROC AUC is:  0.998871331828\n"
     ]
    }
   ],
   "source": [
    "# Predicting on test\n",
    "pred_test = grid_search_model_class_ada.predict(X_test)\n",
    "\n",
    "# Getting performance stats\n",
    "display(pd.crosstab(pred_test,y_test,rownames = [\"Predicted_class\"], colnames = [\"Actual_class\"]))\n",
    "print \"The accuracy is: \", accuracy_score(y_test, pred_test),\"\\n\"\n",
    "print \"The ROC AUC is: \",roc_auc_score(y_test, grid_search_model_class_ada.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Point out one advantage and one disadvantage of Random Forest compared to GBDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages - Random forests are much easier to tune, have a lower tendency to overfit and can be parallelized.\n",
    "\n",
    "Disadvantages - With correct tuning parameters, GBDT generally gives better accuracy when compared to random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 3 - Matrix Factorization for Rating Prediction (20pts)\n",
    "\n",
    "The movielens dataset contains 1 million movie ratings from several thousand users. We will be using *k*-rank matrix factorization to estimate this dataset as the product $X=UV^T$, where *U* and *V* have only $k$ columns.\n",
    "\n",
    "1) You can download the movielens 1M dataset from https://datahub.io/dataset/movielens, but for this problem use the data available on Canvas. It has been split into training and test sets, and converted to matrix format where the rows correspond to users and the columns to movies. Note that most of the entries are NaNs, indicating that these ratings are missing. An extra file, lens1m_361M_titles.csv, has been added so you can check out specific movies if you're curious.\n",
    "\n",
    "2) Scikit-learn is a little behind for recommender systems, and doesn't have any method to factorize matrices with missing data. Which means you get to code it! Slide 22 of the 'apa large scale learning' lecture notes has the equations for stochastic gradient descent on *U* and *V*. You will have to:\n",
    "* Set up initial guesses for the *U* and *V* matrices. I suggest small random values.\n",
    "* Find a suitable learning rate for the descent. A learning rate that is too large will probably blow up, like in HW3 problem 1.\n",
    "* Come up with a stopping policy\n",
    "* Code the descent algorithm (5 pts)\n",
    "\n",
    "3) Using your SGD algorithm, apply 2-rank matrix factorization on the filled training matrix. Calculate the RMSE of this model on the training data and on the test data (separately). The optimal score on the training data is around .86 RMSE; your version of gradient descent must go at least below .91 RMSE. (5 pts)\n",
    "\n",
    "4) You should notice some overfitting. Because matrix factorization learns separate scores for each movie, a movie with very few reviews may be easily overfit. You may want to only predict ratings when you have enough information to reach a good conclusion. Recalculate the RMSE on the test data, specifically for movies with at least 50 reviews (don't retrain the models). Also report the percent of movies that are still included (after cutting those with < 50 reviews), and the percent of test ratings that are still included. (5 pts)\n",
    "\n",
    "5) Repeat steps 3 and 4 with 5-rank factorization. Display training and test RMSE. (5 pts)\n",
    "\n",
    "Hints:  \n",
    "The numpy function *nanmean* is helpful for RMSE calculation.  \n",
    "The descent algorithm will probably run for at least several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#titles = pd.read_csv('lens1m_361M_titles.csv')\n",
    "test_X = np.load('lens1m_361M_test.npy')\n",
    "train_X = np.load('lens1m_361M_train.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. initially filled u and v with random value with the help of random generator library python\n",
    "2. we did not choose a very low value of alpha ( learning rate ) nor a very high value of alpha. We decided to go with 0.002 for 2-rank and 0.01 for 5-rank since it seems to be low, but not low enough and high, but not high enough. \n",
    "3. our stopping policy is when the rmse goes below 0.91 ( which is the worst case value ) or when the number of iterations have gone beyond 10, whichever is lower. We chose 10 iterations because it seemed to be optimal and manages to converge to give good enough rmse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd(x,u,v,k,alpha):\n",
    "    for i in range(0,10):\n",
    "        for j in range(len(x)):\n",
    "            for jter in range(len(x[j])):\n",
    "                if x[j][jter] > 0:\n",
    "                    for kter in range(0,k):\n",
    "                        integral_1 = 2 * alpha * (x[j][jter] - np.dot(u[j,:],v[:,jter])) * v[kter][jter]\n",
    "                        integral_2 = 2 * alpha * (x[j][jter] - np.dot(u[j,:],v[:,jter])) * u[j][kter]\n",
    "                        u[j][kter] = u[j][kter] + integral_1\n",
    "                        v[kter][jter] = v[kter][jter] + integral_2\n",
    "        \n",
    "        new_x = np.dot(u,v)\n",
    "        if np.sqrt(np.nanmean(np.square(np.subtract(train_X,new_x)))) < 0.91:\n",
    "            return u,v\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return u,v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u = np.random.rand(len(train_X),2)\n",
    "v = np.random.rand(len(train_X[0]),2)\n",
    "v = v.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_u,new_v = sgd(train_X,u,v,2,0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_x = np.dot(new_u,new_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90914050466342311"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.nanmean(np.square(np.subtract(train_X,new_x))))\n",
    "#rmse training for 2-rank factorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_rmse = []\n",
    "test_rating_count = 0\n",
    "for i in range(3952):\n",
    "    if np.count_nonzero(~np.isnan(test_X[:,i])) >= 50:\n",
    "        test_rating_count+=np.count_nonzero(~np.isnan(test_X[:,i]))\n",
    "        sum_rmse.append(np.square(np.subtract(test_X[:,i],new_x[:,i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmse_test = np.sqrt(np.nanmean(sum_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91433753482028801"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rmse test for 2-rank factorization with greater than 50 movie reviews\n",
    "rmse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.794534412955464"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#percent of movies retained :\n",
    "\n",
    "1.0*len(sum_rmse)*100/3952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = []\n",
    "test_rating_count = 0\n",
    "for i in range(3952):\n",
    "    if np.count_nonzero(~np.isnan(test_X[:,i])) >= 50:\n",
    "        index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8119394927065317"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test rating count retained as a fraction of total ratings : \n",
    "1.0*(np.count_nonzero(~np.isnan(test_X[:,index])))/np.count_nonzero(~np.isnan(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repeating with rank 5 factorization : \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u = np.random.rand(len(train_X),5)\n",
    "v = np.random.rand(len(train_X[0]),5)\n",
    "v = v.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_u,new_v = sgd(train_X,u,v,5,0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('training rmse', 0.90606213063938978)\n"
     ]
    }
   ],
   "source": [
    "new_x = np.dot(new_u,new_v)\n",
    "print(\"training rmse\", np.sqrt(np.nanmean(np.square(np.subtract(train_X,new_x)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_rmse = []\n",
    "test_rating_count = 0\n",
    "for i in range(3952):\n",
    "    if np.count_nonzero(~np.isnan(test_X[:,i])) >= 50:\n",
    "        test_rating_count+=np.count_nonzero(~np.isnan(test_X[:,i]))\n",
    "        sum_rmse.append(np.square(np.subtract(test_X[:,i],new_x[:,i])))\n",
    "rmse_test = np.sqrt(np.nanmean(sum_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test rmse', 0.93059455524253476)\n"
     ]
    }
   ],
   "source": [
    "#test rmse\n",
    "print(\"test rmse\",rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
